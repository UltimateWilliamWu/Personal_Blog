{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Bayesian methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last revision: Mon 08 Jun 2024 by Peiyu Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "|   Package      | Version     |\n",
    "| -----------    | ----------- |\n",
    "| `matplotlib`   | `3.9.0`     |\n",
    "| `numpy`        | `1.26.4`    |\n",
    "| `pandas`       | `2.2.2`     |\n",
    "| `scikit-learn` | `1.5.0`     |\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We start with a quick refresher on what a classification problem is.\n",
    "\n",
    "### The classification problem\n",
    "For a classification problem, for some population $X$ and a finite set of labels $Y$, we are interested in fitting/finding mapping function $h: X \\to Y$ that matches some relation between $X$ and $Y$. In the case where $Y = \\{0, 1\\}$, we say that the problem is a binary classification problem. \n",
    "\n",
    "Particularly for this lab,\n",
    "1. We begin by looking at the inner workings of Naive Bayes on binary classification problems and provide an implementation for it.   \n",
    "2. We then take a look at the case of multinomial classification for Naive Bayes and also enable smoothing for our implementation. \n",
    "\n",
    "The dataset we will be using is the real data set on the sinking of the Titanic. For a description of the variables and more information on the data please read [here](https://www.kaggle.com/c/titanic-gettingStarted/data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick recap on Naive Bayes\n",
    "\n",
    "Naive Bayes is a straightforward model that leverages Bayes's rule to find the most probable prediction based solely on historical data. Naive Bayes has very few assumptions about the data and it is general enough to apply to most problems. In most cases, Naive Bayes is used as the baseline model for complex problems, hence any more complicated model should do at least better than Naive Bayes! \n",
    "\n",
    "We start with the famous Bayes rule, which tells us that for 2 random variables $X, Y$ we always have that \n",
    "$$\n",
    "    P(y | x) = \\frac{P(x | y)P(y)}{P(x)}\n",
    "$$\n",
    "which \n",
    "* $P(y|x)$ is known as the **posterior**,\n",
    "* $P(x | y)$ is known as **likelihood**,\n",
    "* $P(y)$ is known as the **prior** distribution and\n",
    "* $P(x)$ is the **normalization factor**.\n",
    "\n",
    "Then for a set of historical data $x_1, x_2, \\dots, x_n$, we can apply a few tricks to make our lives a bit easier.\n",
    "1. With especially classification problems, we are only comparing the probability of different classes, therefore the marginal distribution $P(X)$ can be ignored. \n",
    "2. In Bayesian statistics, we usually write likelihood as $L(x_1, x_2, \\dots, x_n|y)$ and since we can assume that each furthermore,\n",
    "3. In most situations, we assume that the sampled random variable is i.i.d hence\n",
    "$$\n",
    "    L(x_1, \\dots, x_n | y) = \\prod_{i = 1}^n P(x_i | y) \\implies  P(y|x_1, \\dots, x_n) \\propto P(y)\\prod_{i = 1}^n P(x_i | y).\n",
    "$$\n",
    "Then for all classes $c_i \\in Y$, we wish to find \n",
    "$$\n",
    "    c^* = \\underset{c_1, \\dots, c_k \\in Y}{\\text{argmax}} P(y|x_1, \\dots, x_n)\n",
    "$$\n",
    "\n",
    "which we call $c^*$ the maximum likelihood hypothesis. Also, note here that $P(x_i | y)$ can be computed from our historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative vs Generative modelling\n",
    "Whenever we consider using probabilistic models in our problem-solving, we usually face a decision to use discriminative or generative models.\n",
    "\n",
    "In **discriminative modelling**, we are only interested in modelling the conditional relationship between the random variables that we need, i.e. the posterior distribution $P(y|x)$.\n",
    "\n",
    "In **generative modelling**, we are interested in modelling the joint distribution $P(x, y)$ which we can then sample from to obtain new data.\n",
    "\n",
    "Suppose that there are $C$ classes for classification label $Y$ and $K$ different possible values for each $x_i$. For generative modelling, referencing the law of marginal distribution we have that\n",
    "$$\n",
    "    P(x, y) = P(y | x) P(x) \\quad \\text{with} \\quad P(x) = \\sum_{i = 1}^C P(y = c_i)P(x_1, \\dots, x_n | y = c_i)\n",
    "$$\n",
    "Therefore we still need to compute the marginal factor $P(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We now have a look at the `titanic` dataset, which tracks the survivalibility guests on the famous sinked ship Titanic. We start by loading the dataset into a dataframe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "titanic_train = pd.read_csv(\"./titanic_train.csv\")\n",
    "titanic_test = pd.read_csv(\"./titanic_test.csv\")\n",
    "\n",
    "feature_set = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# selecting columns\n",
    "X_train = titanic_train[feature_set]\n",
    "X_test = titanic_test[feature_set]\n",
    "y_train = titanic_train[\"Survived\"]\n",
    "y_test = titanic_test[\"Survived\"]\n",
    "\n",
    "\n",
    "print(f\"Shape of training set: {titanic_train.shape}\")\n",
    "print(f\"Shape of test set: {titanic_train.shape}\")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>\n",
    "    \n",
    "#### Exercise 1: \n",
    "Use the template given below or otherwise, implement the Naive Bayes to find the maximum likelihood hypothesis for the test set `titanic_test` using the given features. Comment on the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "class Naive_Bayes():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        What is actually required at this step? ;), do we really need to do anything?\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        1. Find a way to compute the number of classes from y_train, you might find np.unique(.) useful\n",
    "        2. Prior of P(y) should also be computed from y_train as well\n",
    "        3. For each row in X_test, we will need to compute class with the maximum likelihood\n",
    "            * maybe create another function for this, use the vectorized arr[: index] == value to filter\n",
    "                X_train to compute the probabilities\n",
    "            * you might also find np.prod(.) very helpful\n",
    "            * you may also find np.max very useful here as well\n",
    "        '''\n",
    "        pass\n",
    "    def benmark(self, X_test, y_test):\n",
    "        '''\n",
    "        Use your self.predict(.) function to obtain the predicted classes, then use check the result element\n",
    "        by element by using the arr[: index] == value vectorized operator again. Then divide it with the\n",
    "        the length of X_test for the accuracy score.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "nb_model = Naive_Bayes().fit(\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy()\n",
    ")\n",
    "results = nb_model.predict(X_test.to_numpy())\n",
    "accuracy = nb_model.benchmark(X_test.to_numpy(), y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "From our implementation above, we see that in cases when we get $P(x_i | y) = 0$, our entire likelihood inference will decay to 0. Therefore to combat against such instances, we can use a technique called Laplace smoothing. In Laplace smoothing, instead of directly using the data to compute the conditional probabilities, we use the following\n",
    "$$\n",
    " P_L(x_i | y) = \\frac{P(x_i | y) + \\alpha}{N + \\alpha n}\n",
    "$$\n",
    "where \n",
    "* $\\alpha$: a user-chosen hyper-parameter, \n",
    "* $n$: the number of features (like before) and \n",
    "* $N$: the number of data instances. \n",
    "\n",
    "In this way, when $P(x_i | y) = 0$ we get that \n",
    "$$\n",
    " P_L(x_i | y) = \\frac{\\alpha}{N + \\alpha n} > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "From our implementation above, we see that in cases when we get $P(x_i | y) = 0$, our entire likelihood inference will decay to 0. Therefore to combat against such instances, we can use a technique called Laplace smoothing. In Laplace smoothing, instead of directly using the data to compute the conditional probabilities, we use the following\n",
    "$$\n",
    " P_L(x_i | y) = \\frac{P(x_i | y) + \\alpha}{N + \\alpha n}\n",
    "$$\n",
    "where \n",
    "* $\\alpha$: a user-chosen hyper-parameter, \n",
    "* $n$: the number of features (like before) and \n",
    "* $N$: the number of data instances. \n",
    "\n",
    "In this way, when $P(x_i | y) = 0$ we get that \n",
    "$$\n",
    " P_L(x_i | y) = \\frac{\\alpha}{N + \\alpha n} > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>\n",
    "    \n",
    "#### Exercise 2: \n",
    "Basing on your previous implementation (or otherwise), implement \n",
    "1. Laplace smoothing and \n",
    "2. customizable prior values\n",
    "\n",
    "for your Naive Bayes estimator. Then use $\\alpha = 3$ and a custom prior distribution to compute your results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes():\n",
    "    '''\n",
    "    You can copy your code from the previous section, think about which function you need to actually change.\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test, alpha = 0, prior = None):\n",
    "        '''\n",
    "        Does the processing actually change, maybe all it needs just a change in formula or parameters that we use. ;)\n",
    "        '''\n",
    "        pass\n",
    "    def benmark(self, X_test, y_test):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = Naive_Bayes().fit(\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy()\n",
    ")\n",
    "results = nb_model.predict(X_test.to_numpy(), alpha = 3)\n",
    "accuracy = nb_model.benchmark(X_test.to_numpy(), y_test, alpha = 3)\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num0 = np.sum([1 * (d[0] == 0 and d[1] == 0) for d in results])\n",
    "print(f\"Number of predictions with value of 0: {num0}/{len(results)} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Continuous Naive Bayes: LDA and QDA\n",
    "\n",
    "Naive Bayes can also be used to handle continuous features, those models is more commonly known as Linear or Quadratic Discriminant Analysis (LDA or QDA). In LDA,the model is rather straightforward, we start with assuming our likelihood distribution is normally distributed, i.e.\n",
    "$$\n",
    "    P(x | y = c_i) \\sim \\mathcal{N}(\\mu_i, \\Sigma)\n",
    "$$\n",
    "with each different class $c_i$ has a different mean with the same variance $\\Sigma$. We then pick the class $c_i$ that maximizes this the posterior distribution. This is hence equivalent to finding the maximum for the log of posterior\n",
    "$$\n",
    "\\ln P(x | y = c_i) = -\\frac{1}{2} (x - \\mu_{c_i})^T \\Sigma^{-1} (x - \\mu_{c_i}) + \\ln P(y = c_i) + C\n",
    "$$\n",
    "which \n",
    "1. $\\mu_{c_i}$ is the mean for each class which are all assumed to be different and\n",
    "2. $C$ is a constant term from the normal distribution and the normalizing factor $P(x)$.\n",
    "\n",
    "For QDA, the expression changes to \n",
    "$$\n",
    "\\ln P(x | y = c_i) = -\\frac{1}{2} \\ln|\\Sigma_{c_i}| -\\frac{1}{2} (x - \\mu_{c_i})^T \\Sigma_{c_i}^{-1} (x - \\mu_{c_i}) + \\ln P(y = c_i) + C\n",
    "$$\n",
    "since each $\\Sigma_{c_i}$ of different classes are no longer assumed to be the same.\n",
    "We will here omit the derivation as it is beyond the scope of this course, but you have a read about it [here](https://en.wikipedia.org/wiki/Linear_discriminant_analysis). \n",
    "\n",
    "We now look into how to apply LDA/QDA directly using the `GaussianNB` class from `sklearn`. The data below is generated by a ```Gaussian mixture model```. For each class there is a separate 2-dimensional Gaussian distribution over the features `x1`, `x2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(self,mean0,cov0,mean1,cov1):\n",
    "        \"\"\" construct a mixture of two gaussians. mean0 is 2x1 vector\n",
    "            of means for class 0, cov0 is 2x2 covariance matrix for class 0.\n",
    "        Similarly for class 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.mean0 = mean0\n",
    "        self.mean1 = mean1\n",
    "        self.cov0 = cov0\n",
    "        self.cov1 = cov1\n",
    "        self.rv0 = multivariate_normal(mean0, cov0)\n",
    "        self.rv1 = multivariate_normal(mean1, cov1)\n",
    "\n",
    "    def plot(self,data=None):\n",
    "        x1 = np.linspace(-4,4,100)\n",
    "        x2 = np.linspace(-4,4,100)\n",
    "        X1,X2 = np.meshgrid(x1,x2)\n",
    "        pos = np.empty(X1.shape+(2,))\n",
    "        pos[:,:,0] = X1\n",
    "        pos[:,:,1]= X2\n",
    "        a = self.rv1.pdf(pos)/self.rv0.pdf(pos)\n",
    "\n",
    "        if data:\n",
    "            nplots = 4\n",
    "        else:\n",
    "            nplots = 3\n",
    "\n",
    "        fig, ax = plt.subplots(1,nplots,figsize = (5*nplots,5))\n",
    "        [ax[i].spines['left'].set_position('zero') for i in range(0,nplots)]\n",
    "        [ax[i].spines['right'].set_color('none') for i in range(0,nplots)]\n",
    "        [ax[i].spines['bottom'].set_position('zero') for i in range(0,nplots)]\n",
    "        [ax[i].spines['top'].set_color('none') for i in range(0,nplots)]\n",
    "\n",
    "        ax[0].set_title(\"p(x1,x2|y = 1\")\n",
    "        ax[1].set_title(\"p(x1,x2|y = 0\")\n",
    "        ax[2].set_title(\"P(y = 1|x1,x2)\")\n",
    "        [ax[i].set_xlim([-4,4]) for i in range(0,3)]\n",
    "        [ax[i].set_ylim([-4,4]) for i in range(0,3)]\n",
    "\n",
    "        cn = ax[0].contourf(x1,x2,self.rv1.pdf(pos))\n",
    "        cn2 = ax[1].contourf(x1,x2,self.rv0.pdf(pos))\n",
    "        z = a/(1.0+a)\n",
    "        cn3 = ax[2].contourf(x1,x2,z)\n",
    "        ct = ax[2].contour(cn3,levels=[0.5])\n",
    "        ax[2].clabel(ct)\n",
    "\n",
    "\n",
    "        if data:\n",
    "            X, Y = data\n",
    "            colors = [\"blue\" if target < 1 else \"red\" for target in Y]\n",
    "            colors = np.array(colors)\n",
    "            x = X[:,0]\n",
    "            y = X[:,1]\n",
    "            # import IPython; IPython.embed()\n",
    "            yis1 = np.where(Y==1)[0]\n",
    "            yis0 = np.where(Y!=1)[0]\n",
    "            ax[3].set_title(\"Samples colored by class\")\n",
    "            ax[3].scatter(x,y,s=30,c=colors,alpha=.5)\n",
    "            ax[0].scatter(x[yis1],y[yis1],s=5,c=colors[yis1],alpha=.3)\n",
    "            ax[1].scatter(x[yis0],y[yis0],s=5,c=colors[yis0],alpha=.3)\n",
    "            ax[2].scatter(x,y,s=5,c=colors,alpha=.3)\n",
    "        plt.show()\n",
    "\n",
    "    def sample(self,n_samples,py,plot=False):\n",
    "        \"\"\"\n",
    "        samples Y according to py and corresponding features x1,x2\n",
    "        according to the gaussian for the corresponding class\n",
    "        \"\"\"\n",
    "        Y = bernoulli.rvs(py,size=n_samples)\n",
    "        X = np.zeros((n_samples,2))\n",
    "        for i in range(n_samples):\n",
    "            if Y[i] == 1:\n",
    "                X[i,:] = self.rv1.rvs()\n",
    "            else:\n",
    "                X[i,:] = self.rv0.rvs()\n",
    "        if plot:\n",
    "            self.plot(data=(X,Y))\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates some data from a Gaussian Mixture Model.\n",
    "mean0 = [-1,-1]  # the mean of the gaussian for class 0\n",
    "mean1 = [1,1] # the mean of the gaussian for class 1\n",
    "cov0 = [[.5, .28], [.28, .5]] # the covariance matrix for class 0\n",
    "cov1 = [[1, -.8], [-.8, 1]] # the covariance matrix for class 1\n",
    "mixture = GaussianMixture(mean0,cov0,mean1,cov1)\n",
    "mX,mY = mixture.sample(500,0.5,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>\n",
    "    \n",
    "#### Exercise 3: \n",
    "Fit a Gaussian Naive Bayes model using the `GaussianNB` from the `sklearn.naive_bayes` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## start coding here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
